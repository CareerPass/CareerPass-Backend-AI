# voice_ai.py
# ================================================================
# ìŒì„± STT ì„œë²„ (FastAPI + OpenAI Whisper)
# ================================================================

import os
import json
from io import BytesIO  # âœ… ì¶”ê°€

from fastapi import FastAPI, UploadFile, File, Form, HTTPException, APIRouter
from fastapi.responses import Response
from pydantic import BaseModel
import openai
from openai import OpenAI

# -----------------------------
# 0. í™˜ê²½ ë³€ìˆ˜ / OpenAI ì„¤ì •
# -----------------------------

voice_router = APIRouter()

VOICE_KEY = os.environ.get("QUESTION_VOICE_OPENAI_KEY")

try:
    client = OpenAI(api_key=VOICE_KEY)
    print("Voice Router OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
except Exception:
    print("OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ì„ì€ Mock ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    client = None

# -----------------------------
# 1. FastAPI ì•± & CORS
# -----------------------------
# app = FastAPI(title="CareerPass Voice STT")

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# -----------------------------
# 2. ì‘ë‹µ DTO (STT ê²°ê³¼)
# -----------------------------
class SttResult(BaseModel):
    answerText: str  # STT ê²°ê³¼ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜

# -----------------------------
# 3. í—¬ìŠ¤ì²´í¬ & íŒŒë¹„ì½˜
# -----------------------------
@voice_router.get("/health")
async def health():
    return {"ok": True}

@voice_router.get("/favicon.ico")
async def favicon():
    return Response(status_code=204)

# -----------------------------
# 4. í•µì‹¬ API: /analyze
# -----------------------------
@voice_router.post("/analyze", response_model=SttResult)
async def analyze(
    meta: str = Form(...),
    file: UploadFile = File(...),
):
    """
    ğŸ§ ìŒì„± íŒŒì¼ì„ Whisperì— ë³´ë‚´ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    - meta: ì¸í„°ë·° / ì§ˆë¬¸ ì •ë³´ (ë°±ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ìš©ë„)
    - file: .m4a / .mp3 / .wav / .webm / .ogg ë“±
    """

    # 1) meta JSON íŒŒì‹±
    try:
        meta_obj = json.loads(meta)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"invalid meta json: {e}")

    interview_id = meta_obj.get("interviewId")
    question_id = meta_obj.get("questionId")
    user_id = meta_obj.get("userId")  # ì—†ì–´ë„ ë¨ (null í—ˆìš©)

    # 2) íŒŒì¼ ê¸°ë³¸ ê²€ì¦
    if not file or not file.filename:
        raise HTTPException(status_code=400, detail="file missing")

    lower_name = file.filename.lower()
    if not lower_name.endswith((".m4a", ".mp3", ".wav", ".webm", ".ogg")):
        raise HTTPException(status_code=400, detail="unsupported audio type")

    # 3) Whisper í˜¸ì¶œ (ì‹¤ì œ STT)
    try:
        # âœ… ì—…ë¡œë“œ íŒŒì¼ ë°”ì´íŠ¸ ì½ì–´ì„œ BytesIOë¡œ ê°ì‹¸ê¸°
        contents = await file.read()
        audio_bytes = BytesIO(contents)
        audio_bytes.name = file.filename  # ğŸ”¥ ì—¬ê¸°ì„œ í™•ì¥ì í¬í•¨ ì´ë¦„ì„ ë‹¬ì•„ì¤Œ

        transcription = openai.audio.transcriptions.create(
            model="gpt-4o-mini-transcribe",  # ë˜ëŠ” "whisper-1"
            file=audio_bytes,
            language="ko",
        )

        # SDK ë²„ì „ì— ë”°ë¼ text ì†ì„±ì´ ìˆê±°ë‚˜, ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ
        text = getattr(transcription, "text", None) or str(transcription)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Whisper í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    # 4) ìµœì¢… ì‘ë‹µ
    return SttResult(answerText=text)


# ë¡œì»¬ ì‹¤í–‰ìš© ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
#if __name__ == "__main__":
#    import uvicorn
#    uvicorn.run("voice_ai:app", host="0.0.0.0", port=5001, reload=True)